02/25/2020 15:17:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
02/25/2020 15:17:40 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v2-config.json from cache at /home/ijon/.cache/torch/transformers/b3eed512e24335a76694282193217608ead013caa55330de3ff236d1f5695e6c.58c1d03602d9707494c5ff902d07817fd2b4ed6a589b2b062364aed4ae3d3765
02/25/2020 15:17:40 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "finetuning_task": "task4a",
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 16384,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "layers_to_keep": [],
  "max_position_embeddings": 512,
  "net_structure_type": 0,
  "num_attention_heads": 64,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_memory_blocks": 0,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30000
}

02/25/2020 15:17:40 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v2-spiece.model from cache at /home/ijon/.cache/torch/transformers/094b3b4d4ab5e624ae6ba8654d88cdb99b2d5a813b323295c37d58680a1c4127.c81d4deb77aec08ce575b7a39a989a79dd54f321bfb82c2b54dd35f52f8182cf
02/25/2020 15:17:41 - WARNING - transformers.modeling_utils -   There is currently an upstream reproducibility issue with ALBERT v2 models. Please see https://github.com/google-research/google-research/issues/119 for more information.
02/25/2020 15:17:41 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v2-pytorch_model.bin from cache at /home/ijon/.cache/torch/transformers/c8f990f22da3ddf461b7e0d30a079014b20ad2859f352a9f18421485f63a69e7.9ac42d6fae7d18840d74eaf2a6d817700ffdd5af9ae1a12c3e96e239e23f76f4
02/25/2020 15:18:06 - INFO - transformers.modeling_utils -   Weights of AlbertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
02/25/2020 15:18:06 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']
02/25/2020 15:18:21 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/mnt/minerva1/nlp/projects/counterfactual/semeval/4/data/new', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=16, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=210, max_steps=-1, model_name_or_path='albert-xxlarge-v2', model_type='albert', n_gpu=1, no_cuda=False, num_train_epochs=2.0, output_dir='models_bert/albert_4a_new_dev_last_try', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=3, per_gpu_train_batch_size=3, save_steps=50, seed=5431, server_ip='', server_port='', task_name='task4a', tokenizer_name='', warmup_steps=0, weight_decay=0.0)
02/25/2020 15:18:21 - INFO - __main__ -   Loading features from cached file /mnt/minerva1/nlp/projects/counterfactual/semeval/4/data/new/cached_train_albert-xxlarge-v2_210_task4a
02/25/2020 15:18:23 - INFO - __main__ -   ***** Running training *****
02/25/2020 15:18:23 - INFO - __main__ -     Num examples = 10000
02/25/2020 15:18:23 - INFO - __main__ -     Num Epochs = 2
02/25/2020 15:18:23 - INFO - __main__ -     Instantaneous batch size per GPU = 3
02/25/2020 15:18:23 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 48
02/25/2020 15:18:23 - INFO - __main__ -     Gradient Accumulation steps = 16
02/25/2020 15:18:23 - INFO - __main__ -     Total optimization steps = 416
Epoch:   0%|          | 0/2 [00:00<?, ?it/s]
Iteration:   0%|          | 0/3334 [00:00<?, ?it/s][A
Iteration:   0%|          | 1/3334 [00:00<48:41,  1.14it/s][A
Iteration:   0%|          | 2/3334 [00:01<48:23,  1.15it/s][A
Iteration:   0%|          | 3/3334 [00:02<48:11,  1.15it/s][A
Iteration:   0%|          | 4/3334 [00:03<48:02,  1.16it/s][A
Iteration:   0%|          | 5/3334 [00:04<47:58,  1.16it/s][A
Iteration:   0%|          | 6/3334 [00:05<47:57,  1.16it/s][A
Iteration:   0%|          | 7/3334 [00:06<47:56,  1.16it/s][A
Iteration:   0%|          | 8/3334 [00:06<47:53,  1.16it/s][A
Iteration:   0%|          | 9/3334 [00:07<47:52,  1.16it/s][A
Iteration:   0%|          | 10/3334 [00:08<47:51,  1.16it/s][A
Iteration:   0%|          | 11/3334 [00:09<47:50,  1.16it/s][A
Iteration:   0%|          | 12/3334 [00:10<47:51,  1.16it/s][A
Iteration:   0%|          | 13/3334 [00:11<47:51,  1.16it/s][A
Iteration:   0%|          | 14/3334 [00:12<47:52,  1.16it/s][A
Iteration:   0%|          | 15/3334 [00:12<47:52,  1.16it/s][A
Iteration:   0%|          | 16/3334 [00:13<48:18,  1.14it/s][A
Iteration:   1%|          | 17/3334 [00:14<48:34,  1.14it/s][A
Iteration:   1%|          | 18/3334 [00:15<48:20,  1.14it/s][A
Iteration:   1%|          | 19/3334 [00:16<48:11,  1.15it/s][A
Iteration:   1%|          | 20/3334 [00:17<48:04,  1.15it/s][A
Iteration:   1%|          | 21/3334 [00:18<47:59,  1.15it/s][A
Iteration:   1%|          | 22/3334 [00:19<47:56,  1.15it/s][A
Iteration:   1%|          | 23/3334 [00:19<47:55,  1.15it/s][A